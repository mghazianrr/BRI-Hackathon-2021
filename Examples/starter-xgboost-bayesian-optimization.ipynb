{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/bri-data-hackathon-pa/sample_submission.csv\n/kaggle/input/bri-data-hackathon-pa/data_description.csv\n/kaggle/input/bri-data-hackathon-pa/train.csv\n/kaggle/input/bri-data-hackathon-pa/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Hello everyone!\n\n**This notebook presents a straightforward code to tune hyperparameter of LGBM, CAT, and XGB with Bayesian Optimization. It is like GridSearchCV and RandomizedSearchCV.**\n\nGridSearchCV searches for all combinations of parameters, and it could take a very long time. Not very efficient. RandomizedSearchCV searches the combination randomly. Somehow the algorithm can skip the optimal parameter, especially if the search grid is enormous. Bayesian Optimization is a smarter method to tune the hyperparameter. I won't discuss the theory behind it in this notebook as it is straightforward.\n\nIf you have any questions regarding the code, please comment below. I will update the notebook accordingly.\n\n**Please do upvote the notebook if this notebook helps you as it will be a benchmark for me to do more work in the future. Thank you :)**\n\n**Note: I do not do the feature engineering here, so the result may sub-optimal**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read train and test set\ntrain = pd.read_csv(\"/kaggle/input/bri-data-hackathon-pa/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/bri-data-hackathon-pa/test.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train set into dependent variables and independent variable\ny = train['Best Performance']\nX = train.drop('Best Performance', axis=1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to dummy variables\nX = pd.get_dummies(X)\ntest = pd.get_dummies(test)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the common features between train and test set and use it to filter the train and test set\ncommon = list(set(X.columns).intersection(set(test.columns)))\nX = X[common]\ntest = test[common]","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost - Cross Validation Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\n# fit model to training data\nmodel = xgb.XGBRFClassifier(n_estimators=1000, random_state=1245)\n# cross validation score\nscore = cross_val_score(model, X, y, cv=5, scoring=\"roc_auc\", n_jobs=-1)\nprint(\"XGB ROC-AUC Mean Score: \", np.mean(score))","execution_count":7,"outputs":[{"output_type":"stream","text":"XGB ROC-AUC Mean Score:  0.5709700832656107\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Tuning with Bayesian Optimization\n\nNow we will use Bayesian Optimization to tune the hyperparameter. Our goal is to maximize AUC.\n\nYou can also adjust what parameter you want to tune and the range of hyperparameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to special data format\n# https://xgboost.readthedocs.io/en/latest/python/python_intro.html\ndtrain = xgb.DMatrix(X, y, feature_names=X.columns.values)\n\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'binary:logistic',\n    'eval_metric':'auc',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=5)\n    return scores['test-auc-mean'].iloc[-1]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 10),\n  'subsample':(0.5, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (2, 15),\n  'learning_rate': (0.01, 0.5)\n}","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = BayesianOptimization(hyp_xgb, pds, random_state=1)\noptimizer.maximize(init_points=4, n_iter=25)","execution_count":11,"outputs":[{"output_type":"stream","text":"|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | subsample |\n-------------------------------------------------------------------------------------------------\n| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5665  \u001b[0m | \u001b[0m 0.4753  \u001b[0m | \u001b[0m 7.203   \u001b[0m | \u001b[0m 0.01006 \u001b[0m | \u001b[0m 5.93    \u001b[0m | \u001b[0m 5.495   \u001b[0m | \u001b[0m 0.5462  \u001b[0m |\n| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5762  \u001b[0m | \u001b[95m 0.2676  \u001b[0m | \u001b[95m 3.456   \u001b[0m | \u001b[95m 0.2044  \u001b[0m | \u001b[95m 9.005   \u001b[0m | \u001b[95m 10.13   \u001b[0m | \u001b[95m 0.8426  \u001b[0m |\n| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5689  \u001b[0m | \u001b[0m 0.284   \u001b[0m | \u001b[0m 8.781   \u001b[0m | \u001b[0m 0.02342 \u001b[0m | \u001b[0m 10.72   \u001b[0m | \u001b[0m 10.09   \u001b[0m | \u001b[0m 0.7793  \u001b[0m |\n| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5639  \u001b[0m | \u001b[0m 0.2263  \u001b[0m | \u001b[0m 1.981   \u001b[0m | \u001b[0m 0.4024  \u001b[0m | \u001b[0m 14.59   \u001b[0m | \u001b[0m 8.328   \u001b[0m | \u001b[0m 0.8462  \u001b[0m |\n| \u001b[0m 5       \u001b[0m | \u001b[0m 0.566   \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 3.604   \u001b[0m | \u001b[0m 0.4254  \u001b[0m | \u001b[0m 8.613   \u001b[0m | \u001b[0m 10.07   \u001b[0m | \u001b[0m 0.6585  \u001b[0m |\n| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5708  \u001b[0m | \u001b[0m 0.6379  \u001b[0m | \u001b[0m 3.184   \u001b[0m | \u001b[0m 0.3349  \u001b[0m | \u001b[0m 7.829   \u001b[0m | \u001b[0m 5.927   \u001b[0m | \u001b[0m 0.6515  \u001b[0m |\n| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5758  \u001b[0m | \u001b[0m 0.9075  \u001b[0m | \u001b[0m 5.296   \u001b[0m | \u001b[0m 0.03891 \u001b[0m | \u001b[0m 12.88   \u001b[0m | \u001b[0m 6.857   \u001b[0m | \u001b[0m 0.5926  \u001b[0m |\n| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5725  \u001b[0m | \u001b[0m 0.4686  \u001b[0m | \u001b[0m 1.223   \u001b[0m | \u001b[0m 0.3677  \u001b[0m | \u001b[0m 13.29   \u001b[0m | \u001b[0m 14.27   \u001b[0m | \u001b[0m 0.6975  \u001b[0m |\n| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5583  \u001b[0m | \u001b[0m 0.9251  \u001b[0m | \u001b[0m 0.9461  \u001b[0m | \u001b[0m 0.4935  \u001b[0m | \u001b[0m 14.31   \u001b[0m | \u001b[0m 3.227   \u001b[0m | \u001b[0m 0.727   \u001b[0m |\n| \u001b[0m 10      \u001b[0m | \u001b[0m 0.566   \u001b[0m | \u001b[0m 0.7246  \u001b[0m | \u001b[0m 5.736   \u001b[0m | \u001b[0m 0.1713  \u001b[0m | \u001b[0m 11.23   \u001b[0m | \u001b[0m 16.56   \u001b[0m | \u001b[0m 0.5152  \u001b[0m |\n| \u001b[95m 11      \u001b[0m | \u001b[95m 0.5793  \u001b[0m | \u001b[95m 0.2816  \u001b[0m | \u001b[95m 2.937   \u001b[0m | \u001b[95m 0.08802 \u001b[0m | \u001b[95m 5.643   \u001b[0m | \u001b[95m 16.8    \u001b[0m | \u001b[95m 0.8532  \u001b[0m |\n| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5762  \u001b[0m | \u001b[0m 0.1042  \u001b[0m | \u001b[0m 8.21    \u001b[0m | \u001b[0m 0.2611  \u001b[0m | \u001b[0m 8.977   \u001b[0m | \u001b[0m 15.55   \u001b[0m | \u001b[0m 0.9726  \u001b[0m |\n| \u001b[0m 13      \u001b[0m | \u001b[0m 0.57    \u001b[0m | \u001b[0m 0.1307  \u001b[0m | \u001b[0m 7.185   \u001b[0m | \u001b[0m 0.4332  \u001b[0m | \u001b[0m 5.906   \u001b[0m | \u001b[0m 17.39   \u001b[0m | \u001b[0m 0.711   \u001b[0m |\n| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5762  \u001b[0m | \u001b[0m 0.9083  \u001b[0m | \u001b[0m 0.8642  \u001b[0m | \u001b[0m 0.2842  \u001b[0m | \u001b[0m 5.312   \u001b[0m | \u001b[0m 15.84   \u001b[0m | \u001b[0m 0.6189  \u001b[0m |\n| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5791  \u001b[0m | \u001b[0m 0.99    \u001b[0m | \u001b[0m 7.459   \u001b[0m | \u001b[0m 0.1681  \u001b[0m | \u001b[0m 6.865   \u001b[0m | \u001b[0m 8.545   \u001b[0m | \u001b[0m 0.7904  \u001b[0m |\n| \u001b[95m 16      \u001b[0m | \u001b[95m 0.585   \u001b[0m | \u001b[95m 0.7825  \u001b[0m | \u001b[95m 2.038   \u001b[0m | \u001b[95m 0.1244  \u001b[0m | \u001b[95m 9.608   \u001b[0m | \u001b[95m 16.03   \u001b[0m | \u001b[95m 0.9829  \u001b[0m |\n| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5804  \u001b[0m | \u001b[0m 0.8865  \u001b[0m | \u001b[0m 9.922   \u001b[0m | \u001b[0m 0.2004  \u001b[0m | \u001b[0m 13.22   \u001b[0m | \u001b[0m 16.46   \u001b[0m | \u001b[0m 0.8625  \u001b[0m |\n| \u001b[0m 18      \u001b[0m | \u001b[0m 0.564   \u001b[0m | \u001b[0m 0.1157  \u001b[0m | \u001b[0m 3.313   \u001b[0m | \u001b[0m 0.2382  \u001b[0m | \u001b[0m 5.732   \u001b[0m | \u001b[0m 13.83   \u001b[0m | \u001b[0m 0.6987  \u001b[0m |\n| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5799  \u001b[0m | \u001b[0m 0.6041  \u001b[0m | \u001b[0m 5.896   \u001b[0m | \u001b[0m 0.3652  \u001b[0m | \u001b[0m 11.86   \u001b[0m | \u001b[0m 10.42   \u001b[0m | \u001b[0m 0.8302  \u001b[0m |\n| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5729  \u001b[0m | \u001b[0m 0.1688  \u001b[0m | \u001b[0m 6.705   \u001b[0m | \u001b[0m 0.3486  \u001b[0m | \u001b[0m 10.91   \u001b[0m | \u001b[0m 7.758   \u001b[0m | \u001b[0m 0.8086  \u001b[0m |\n| \u001b[95m 21      \u001b[0m | \u001b[95m 0.5855  \u001b[0m | \u001b[95m 0.8485  \u001b[0m | \u001b[95m 0.07362 \u001b[0m | \u001b[95m 0.0292  \u001b[0m | \u001b[95m 12.5    \u001b[0m | \u001b[95m 4.537   \u001b[0m | \u001b[95m 0.8782  \u001b[0m |\n| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5658  \u001b[0m | \u001b[0m 0.1833  \u001b[0m | \u001b[0m 9.937   \u001b[0m | \u001b[0m 0.2367  \u001b[0m | \u001b[0m 3.811   \u001b[0m | \u001b[0m 8.027   \u001b[0m | \u001b[0m 0.5583  \u001b[0m |\n| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5718  \u001b[0m | \u001b[0m 0.2403  \u001b[0m | \u001b[0m 0.6637  \u001b[0m | \u001b[0m 0.4578  \u001b[0m | \u001b[0m 3.277   \u001b[0m | \u001b[0m 8.299   \u001b[0m | \u001b[0m 0.8203  \u001b[0m |\n| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5769  \u001b[0m | \u001b[0m 0.4651  \u001b[0m | \u001b[0m 7.331   \u001b[0m | \u001b[0m 0.4297  \u001b[0m | \u001b[0m 14.88   \u001b[0m | \u001b[0m 17.53   \u001b[0m | \u001b[0m 0.6249  \u001b[0m |\n| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5631  \u001b[0m | \u001b[0m 0.113   \u001b[0m | \u001b[0m 6.407   \u001b[0m | \u001b[0m 0.0181  \u001b[0m | \u001b[0m 13.54   \u001b[0m | \u001b[0m 18.96   \u001b[0m | \u001b[0m 0.6135  \u001b[0m |\n| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5742  \u001b[0m | \u001b[0m 0.148   \u001b[0m | \u001b[0m 5.452   \u001b[0m | \u001b[0m 0.2703  \u001b[0m | \u001b[0m 2.435   \u001b[0m | \u001b[0m 7.789   \u001b[0m | \u001b[0m 0.7539  \u001b[0m |\n| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5786  \u001b[0m | \u001b[0m 0.5351  \u001b[0m | \u001b[0m 7.604   \u001b[0m | \u001b[0m 0.4074  \u001b[0m | \u001b[0m 12.15   \u001b[0m | \u001b[0m 17.89   \u001b[0m | \u001b[0m 0.856   \u001b[0m |\n| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5599  \u001b[0m | \u001b[0m 0.4406  \u001b[0m | \u001b[0m 0.8273  \u001b[0m | \u001b[0m 0.2585  \u001b[0m | \u001b[0m 7.518   \u001b[0m | \u001b[0m 3.493   \u001b[0m | \u001b[0m 0.578   \u001b[0m |\n| \u001b[0m 29      \u001b[0m | \u001b[0m 0.5711  \u001b[0m | \u001b[0m 0.7341  \u001b[0m | \u001b[0m 1.338   \u001b[0m | \u001b[0m 0.4011  \u001b[0m | \u001b[0m 10.27   \u001b[0m | \u001b[0m 13.49   \u001b[0m | \u001b[0m 0.7667  \u001b[0m |\n=================================================================================================\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer.max['params']","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"{'colsample_bytree': 0.8485245380480577,\n 'gamma': 0.07362378227392719,\n 'learning_rate': 0.02919903835311307,\n 'max_depth': 12.501698562782558,\n 'min_child_weight': 4.536601691335176,\n 'subsample': 0.8782155860634835}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Instantiate with new hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from above\n# Some params need to be an integer\nparams = {\n    'colsample_bytree': 0.8485245380480577,\n    'gamma': 0.07362378227392719,\n    'learning_rate': 0.02919903835311307,\n    'max_depth': 13,\n    'min_child_weight': 5,\n    'subsample': 0.8782155860634835,\n    'objective': 'binary:logistic',\n    'eval_metric':'auc',\n    'n_jobs':-1\n}","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbr =  xgb.XGBClassifier(**params, random_state=12345, nthread=-1)\nxgbr.fit(X, y)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.8485245380480577,\n              eval_metric='auc', gamma=0.07362378227392719, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.02919903835311307, max_delta_step=0, max_depth=13,\n              min_child_weight=5, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=-1, nthread=-1, num_parallel_tree=1,\n              random_state=12345, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              subsample=0.8782155860634835, tree_method='exact',\n              validate_parameters=1, verbosity=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict the probability using predict_proba\ny_pred = xgbr.predict_proba(test)[:,1]","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/bri-data-hackathon-pa/sample_submission.csv\")\nsubmission.head()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"   index  Best Performance\n0      0          0.131028\n1      1          0.379354\n2      2          0.031798\n3      3          0.285220\n4      4          0.848732","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>Best Performance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.131028</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.379354</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.031798</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.285220</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.848732</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['Best Performance'] = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":19,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}